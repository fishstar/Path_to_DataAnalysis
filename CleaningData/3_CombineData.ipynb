{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining data for analysis\n",
    "\n",
    "## Concatenating data\n",
    "\n",
    "### Combining rows of data\n",
    "\n",
    "The dataset you'll be working with here relates to NYC Uber data. The original dataset has all the originating Uber pickup locations by time and latitude and longitude. For didactic purposes, you'll be working with a very small portion of the actual data.\n",
    "\n",
    "Three DataFrames have been pre-loaded: uber1, which contains data for April 2014, uber2, which contains data for May 2014, and uber3, which contains data for June 2014. Your job in this exercise is to concatenate these DataFrames together such that the resulting DataFrame has the data for all three months.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "uber1 = pd.DataFrame({'Date/Time': ['4/1/2014 0:11:00', '4/1/2014 0:17:00', '4/1/2014 0:21:00', '4/1/2014 0:28:00', '4/1/2014 0:33:00'],\n",
    "                      'Lat': [40.768999999999998, 40.726700000000001, 40.7316, 40.758800000000001, 40.759399999999999],\n",
    "                      'Lon': [-73.954899999999995, -74.034499999999994, -73.987300000000005, -73.977599999999995, -73.972200000000001],\n",
    "                      'Base': ['B02512', 'B02512', 'B02512', 'B02512', 'B02512']    \n",
    "})\n",
    "\n",
    "uber2 = pd.DataFrame({'Date/Time': ['5/1/2014 0:02:00', '5/1/2014 0:06:00', '5/1/2014 0:15:00', '5/1/2014 0:17:00', '5/1/2014 0:17:00'],\n",
    "                      'Lat': [40.752099999999999, 40.6965, 40.746400000000001, 40.746299999999998, 40.759399999999999],\n",
    "                      'Lon': [-73.991399999999999, -73.971500000000006, -73.983800000000002, -74.001099999999994, -73.973399999999998],\n",
    "                      'Base': ['B02512', 'B02512', 'B02512', 'B02512', 'B02512']\n",
    "})\n",
    "\n",
    "uber3 = pd.DataFrame({'Date/Time': ['6/1/2014 0:00:00', '6/1/2014 0:01:00', '6/1/2014 0:04:00', '6/1/2014 0:04:00', '6/1/2014 0:07:00'],\n",
    "                      'Lat': [40.729300000000002, 40.713099999999997, 40.3461, 40.755499999999998, 40.688000000000002],\n",
    "                      'Lon': [-73.992000000000004, -74.009699999999995, -74.661000000000001, -73.9833, -74.183099999999996],\n",
    "                      'Base': ['B02512', 'B02512', 'B02512', 'B02512', 'B02512']\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 4)\n",
      "     Base         Date/Time      Lat      Lon\n",
      "0  B02512  4/1/2014 0:11:00  40.7690 -73.9549\n",
      "1  B02512  4/1/2014 0:17:00  40.7267 -74.0345\n",
      "2  B02512  4/1/2014 0:21:00  40.7316 -73.9873\n",
      "3  B02512  4/1/2014 0:28:00  40.7588 -73.9776\n",
      "4  B02512  4/1/2014 0:33:00  40.7594 -73.9722\n",
      "0  B02512  5/1/2014 0:02:00  40.7521 -73.9914\n",
      "1  B02512  5/1/2014 0:06:00  40.6965 -73.9715\n",
      "2  B02512  5/1/2014 0:15:00  40.7464 -73.9838\n",
      "3  B02512  5/1/2014 0:17:00  40.7463 -74.0011\n",
      "4  B02512  5/1/2014 0:17:00  40.7594 -73.9734\n",
      "0  B02512  6/1/2014 0:00:00  40.7293 -73.9920\n",
      "1  B02512  6/1/2014 0:01:00  40.7131 -74.0097\n",
      "2  B02512  6/1/2014 0:04:00  40.3461 -74.6610\n",
      "3  B02512  6/1/2014 0:04:00  40.7555 -73.9833\n",
      "4  B02512  6/1/2014 0:07:00  40.6880 -74.1831\n"
     ]
    }
   ],
   "source": [
    "# Concatenate uber1, uber2, and uber3: row_concat\n",
    "row_concat = pd.concat([uber1, uber2, uber3])\n",
    "\n",
    "# Print the shape of row_concat\n",
    "print(row_concat.shape)\n",
    "\n",
    "# Print the head of row_concat\n",
    "print(row_concat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining columns of data\n",
    "Think of column-wise concatenation of data as stitching data together from the sides instead of the top and bottom. To perform this action, you use the same pd.concat() function, but this time with the keyword argument axis=1. The default, axis=0, is for a row-wise concatenation.\n",
    "\n",
    "You'll return to the Ebola dataset you worked with briefly in the last chapter. It has been pre-loaded into a DataFrame called ebola_melt. In this DataFrame, the status and country of a patient is contained in a single column. This column has been parsed into a new DataFrame, status_country, where there are separate columns for status and country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ebola_melt = pd.DataFrame({\n",
    "    'Date': ['1/5/2015', '1/4/2015', '1/3/2015', '1/2/2015', '12/31/2014'],\n",
    "    'Day': [289, 288, 287, 286, 284],\n",
    "    'counts': [2776.0, 2775.0, 2769.0, np.nan, 2730.0]\n",
    "})\n",
    "\n",
    "status_country = pd.DataFrame({\n",
    "    'status': ['Cases', 'Cases', 'Cases', 'Cases', 'Cases'],\n",
    "    'country': ['Guinea', 'Guinea', 'Guinea', 'Guinea', 'Guinea']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "         Date  Day  counts country status\n",
      "0    1/5/2015  289  2776.0  Guinea  Cases\n",
      "1    1/4/2015  288  2775.0  Guinea  Cases\n",
      "2    1/3/2015  287  2769.0  Guinea  Cases\n",
      "3    1/2/2015  286     NaN  Guinea  Cases\n",
      "4  12/31/2014  284  2730.0  Guinea  Cases\n"
     ]
    }
   ],
   "source": [
    "# Concatenate ebola_melt and status_country column-wise: ebola_tidy\n",
    "ebola_tidy = pd.concat([ebola_melt, status_country], axis=1)\n",
    "\n",
    "# Print the shape of ebola_tidy\n",
    "print(ebola_tidy.shape)\n",
    "\n",
    "# Print the head of ebola_tidy\n",
    "print(ebola_tidy.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding files that match a pattern\n",
    "\n",
    "You're now going to practice using the glob module to find all csv files in the workspace. In the next exercise, you'll programmatically load them into DataFrames.\n",
    "\n",
    "As Dan showed you in the video, the glob module has a function called glob that takes a pattern and returns a list of the files in the working directory that match that pattern.\n",
    "\n",
    "For example, if you know the pattern is part_ single digit number .csv, you can write the pattern as 'part_?.csv' (which would match part_1.csv, part_2.csv, part_3.csv, etc.)\n",
    "\n",
    "Similarly, you can find all .csv files with '*.csv', or all parts with 'part_*'. The ? wildcard represents any 1 character, and the * wildcard represents any number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uber/uber-raw-data-2014_04.csv', 'uber/uber-raw-data-2014_05.csv', 'uber/uber-raw-data-2014_06.csv']\n",
      "          Date/Time      Lat      Lon    Base\n",
      "0  5/1/2014 0:02:00  40.7521 -73.9914  B02512\n",
      "1  5/1/2014 0:06:00  40.6965 -73.9715  B02512\n",
      "2  5/1/2014 0:15:00  40.7464 -73.9838  B02512\n",
      "3  5/1/2014 0:17:00  40.7463 -74.0011  B02512\n",
      "4  5/1/2014 0:17:00  40.7594 -73.9734  B02512\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = 'uber/*.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Print the file names\n",
    "print(csv_files)\n",
    "\n",
    "# Load the second file into a DataFrame: csv2\n",
    "csv2 = pd.read_csv(csv_files[1])\n",
    "\n",
    "# Print the head of csv2\n",
    "print(csv2.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating and concatenating all matches\n",
    "\n",
    "Now that you have a list of filenames to load, you can load all the files into a list of DataFrames that can then be concatenated.\n",
    "\n",
    "You'll start with an empty list called frames. Your job is to use a for loop to iterate through each of the filenames, read each filename into a DataFrame, and then append it to the frames list.\n",
    "\n",
    "You can then concatenate this list of DataFrames using pd.concat(). Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 4)\n",
      "          Date/Time      Lat      Lon    Base\n",
      "0  4/1/2014 0:11:00  40.7690 -73.9549  B02512\n",
      "1  4/1/2014 0:17:00  40.7267 -74.0345  B02512\n",
      "2  4/1/2014 0:21:00  40.7316 -73.9873  B02512\n",
      "3  4/1/2014 0:28:00  40.7588 -73.9776  B02512\n",
      "4  4/1/2014 0:33:00  40.7594 -73.9722  B02512\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list: frames\n",
    "frames = []\n",
    "\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "uber = pd.concat(frames)\n",
    "\n",
    "# Print the shape of uber\n",
    "print(uber.shape)\n",
    "\n",
    "# Print the head of uber\n",
    "print(uber.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data\n",
    "\n",
    "### 1-to-1 data merge\n",
    "Merging data allows you to combine disparate datasets into a single dataset to do more complex analysis.\n",
    "\n",
    "Here, you'll be using survey data that contains readings that William Dyer, Frank Pabodie, and Valentina Roerich took in the late 1920 and 1930 while they were on an expedition towards Antarctica. The dataset was taken from a sqlite database from the Software Carpentry SQL lesson.\n",
    "\n",
    "Two DataFrames have been pre-loaded: site and visited. Explore them in the IPython Shell and take note of their structure and column names. Your task is to perform a 1-to-1 merge of these two DataFrames using the 'name' column of site and the 'site' column of visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site = pd.DataFrame({'name': ['DR-1','DR-3','MSK-4'],\n",
    "                     'lat': [-49.85, -47.15, -48.87],\n",
    "                     'long': [-128.57, -126.72, -123.40]\n",
    "                    })\n",
    "\n",
    "visited = pd.DataFrame({'ident': [619, 743, 837],\n",
    "                        'site': ['DR-1','DR-3','MSK-4'],\n",
    "                        'dated': ['1927-02-08', '1939-01-07', '1932-01-14']\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lat    long   name       dated  ident   site\n",
      "0 -49.85 -128.57   DR-1  1927-02-08    619   DR-1\n",
      "1 -47.15 -126.72   DR-3  1939-01-07    743   DR-3\n",
      "2 -48.87 -123.40  MSK-4  1932-01-14    837  MSK-4\n"
     ]
    }
   ],
   "source": [
    "# Merge the DataFrames: o2o\n",
    "o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Print o2o\n",
    "print(o2o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-1 data merge\n",
    "In a many-to-one (or one-to-many) merge, one of the values will be duplicated and recycled in the output. That is, one of the keys in the merge is not unique.\n",
    "\n",
    "Here, the two DataFrames site and visited have been pre-loaded once again. Note that this time, visited has multiple entries for the site column. Confirm this by exploring it in the IPython Shell.\n",
    "\n",
    "The .merge() method call is the same as the 1-to-1 merge from the previous exercise, but the data and output will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site = pd.DataFrame({'name': ['DR-1','DR-3','MSK-4'],\n",
    "                     'lat': [-49.85, -47.15, -48.87],\n",
    "                     'long': [-128.57, -126.72, -123.40]\n",
    "                    })\n",
    "\n",
    "visited = pd.DataFrame({'ident': [619, 622, 734, 735, 751, 752, 837, 844],\n",
    "                        'site': ['DR-1', 'DR-1', 'DR-3', 'DR-3', 'DR-3', 'DR-3', 'MSK-4', 'DR-1'],\n",
    "                        'dated': ['1927-02-08', '1927-02-10', '1939-01-07', '1930-01-12', '1930-02-26', np.nan, '1932-01-14', '1932-03-22']\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lat    long   name       dated  ident   site\n",
      "0 -49.85 -128.57   DR-1  1927-02-08    619   DR-1\n",
      "1 -49.85 -128.57   DR-1  1927-02-10    622   DR-1\n",
      "2 -49.85 -128.57   DR-1  1932-03-22    844   DR-1\n",
      "3 -47.15 -126.72   DR-3  1939-01-07    734   DR-3\n",
      "4 -47.15 -126.72   DR-3  1930-01-12    735   DR-3\n",
      "5 -47.15 -126.72   DR-3  1930-02-26    751   DR-3\n",
      "6 -47.15 -126.72   DR-3         NaN    752   DR-3\n",
      "7 -48.87 -123.40  MSK-4  1932-01-14    837  MSK-4\n"
     ]
    }
   ],
   "source": [
    "# Merge the DataFrames: m2o\n",
    "m2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Print m2o\n",
    "print(m2o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-many data merge\n",
    "The final merging scenario occurs when both DataFrames do not have unique keys for a merge. What happens here is that for each duplicated key, every pairwise combination will be created.\n",
    "\n",
    "\n",
    "Here, you'll work with the site and visited DataFrames from before, and a new survey DataFrame. Your task is to merge site and visited as you did in the earlier exercises. You will then merge this merged DataFrame with survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "survey = pd.DataFrame({\n",
    "    'taken': [619, 619, 622, 622, 734, 734, 734, 735, 735, 735, 751, 751, 751, 752, 752, 752, 752, 837, 837, 837, 844],\n",
    "    'person': ['dyer', 'dyer', 'dyer', 'dyer', 'pb', 'lake', 'pb', 'pb', np.nan, np.nan, 'pb', 'pb', 'lake', 'lake', 'lake', 'lake', 'roe', 'lake', 'lake', 'roe', 'roe'],\n",
    "    'quant': ['rad', 'sal', 'rad', 'sal', 'rad', 'sal', 'temp', 'rad', 'sal', 'temp', 'rad', 'temp', 'sal', 'rad', 'sal', 'temp', 'sal', 'rad', 'sal', 'sal', 'rad'],\n",
    "    'reading': [9.8200000000000003, 0.13, 7.7999999999999998, 0.089999999999999997, 8.4100000000000001, 0.050000000000000003, -21.5, 7.2199999999999998, 0.059999999999999998, -26.0, 4.3499999999999996, -18.5, 0.10000000000000001, 2.1899999999999999, 0.089999999999999997, -16.0, 41.600000000000001, 1.46, 0.20999999999999999, 22.5, 11.25]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      lat    long   name       dated  ident   site person quant  reading  \\\n",
      "0  -49.85 -128.57   DR-1  1927-02-08    619   DR-1   dyer   rad     9.82   \n",
      "1  -49.85 -128.57   DR-1  1927-02-08    619   DR-1   dyer   sal     0.13   \n",
      "2  -49.85 -128.57   DR-1  1927-02-10    622   DR-1   dyer   rad     7.80   \n",
      "3  -49.85 -128.57   DR-1  1927-02-10    622   DR-1   dyer   sal     0.09   \n",
      "4  -49.85 -128.57   DR-1  1932-03-22    844   DR-1    roe   rad    11.25   \n",
      "5  -47.15 -126.72   DR-3  1939-01-07    734   DR-3     pb   rad     8.41   \n",
      "6  -47.15 -126.72   DR-3  1939-01-07    734   DR-3   lake   sal     0.05   \n",
      "7  -47.15 -126.72   DR-3  1939-01-07    734   DR-3     pb  temp   -21.50   \n",
      "8  -47.15 -126.72   DR-3  1930-01-12    735   DR-3     pb   rad     7.22   \n",
      "9  -47.15 -126.72   DR-3  1930-01-12    735   DR-3    NaN   sal     0.06   \n",
      "10 -47.15 -126.72   DR-3  1930-01-12    735   DR-3    NaN  temp   -26.00   \n",
      "11 -47.15 -126.72   DR-3  1930-02-26    751   DR-3     pb   rad     4.35   \n",
      "12 -47.15 -126.72   DR-3  1930-02-26    751   DR-3     pb  temp   -18.50   \n",
      "13 -47.15 -126.72   DR-3  1930-02-26    751   DR-3   lake   sal     0.10   \n",
      "14 -47.15 -126.72   DR-3         NaN    752   DR-3   lake   rad     2.19   \n",
      "15 -47.15 -126.72   DR-3         NaN    752   DR-3   lake   sal     0.09   \n",
      "16 -47.15 -126.72   DR-3         NaN    752   DR-3   lake  temp   -16.00   \n",
      "17 -47.15 -126.72   DR-3         NaN    752   DR-3    roe   sal    41.60   \n",
      "18 -48.87 -123.40  MSK-4  1932-01-14    837  MSK-4   lake   rad     1.46   \n",
      "19 -48.87 -123.40  MSK-4  1932-01-14    837  MSK-4   lake   sal     0.21   \n",
      "\n",
      "    taken  \n",
      "0     619  \n",
      "1     619  \n",
      "2     622  \n",
      "3     622  \n",
      "4     844  \n",
      "5     734  \n",
      "6     734  \n",
      "7     734  \n",
      "8     735  \n",
      "9     735  \n",
      "10    735  \n",
      "11    751  \n",
      "12    751  \n",
      "13    751  \n",
      "14    752  \n",
      "15    752  \n",
      "16    752  \n",
      "17    752  \n",
      "18    837  \n",
      "19    837  \n"
     ]
    }
   ],
   "source": [
    "# Merge site and visited: m2m\n",
    "m2m = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Merge m2m and survey: m2m\n",
    "m2m = pd.merge(left=m2m, right=survey, left_on='ident', right_on='taken')\n",
    "\n",
    "# Print the first 20 lines of m2m\n",
    "print(m2m.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
