{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping data \n",
    "## Categoricals and groupby\n",
    "### Grouping by multiple columns\n",
    "\n",
    "In this exercise, you will return to working with the Titanic dataset from Chapter 1 and use .groupby() to analyze the distribution of passengers who boarded the Titanic.\n",
    "\n",
    "The 'pclass' column identifies which class of ticket was purchased by the passenger and the 'embarked' column indicates at which of the three ports the passenger boarded the Titanic. 'S' stands for Southampton, England, 'C' for Cherbourg, France and 'Q' for Queenstown, Ireland.\n",
    "\n",
    "Your job is to first group by the 'pclass' column and count the number of rows in each class using the 'survived' column. You will then group by the 'embarked' and 'pclass' columns and count the number of passengers.\n",
    "\n",
    "The DataFrame has been pre-loaded as titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pclass\n",
      "1    323\n",
      "2    277\n",
      "3    709\n",
      "Name: survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Group titanic by 'pclass'\n",
    "by_class = titanic.groupby('pclass')\n",
    "\n",
    "# Aggregate 'survived' column of by_class by count\n",
    "count_by_class = by_class['survived'].count()\n",
    "\n",
    "# Print count_by_class\n",
    "print(count_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embarked  pclass\n",
      "C         1         141\n",
      "          2          28\n",
      "          3         101\n",
      "Q         1           3\n",
      "          2           7\n",
      "          3         113\n",
      "S         1         177\n",
      "          2         242\n",
      "          3         495\n",
      "Name: survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Group titanic by 'embarked' and 'pclass'\n",
    "by_mult = titanic.groupby(['embarked', 'pclass'])\n",
    "\n",
    "# Aggregate 'survived' column of by_mult by count\n",
    "count_mult = by_mult['survived'].count()\n",
    "\n",
    "# Print count_mult\n",
    "print(count_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by another series\n",
    "In this exercise, you'll use two data sets from Gapminder.org to investigate the average life expectancy (in years) at birth in 2010 for the 6 continental regions. To do this you'll read the life expectancy data per country into one pandas DataFrame and the association between country and region into another.\n",
    "\n",
    "By setting the index of both DataFrames to the country name, you'll then use the region information to group the countries in the life expectancy DataFrame and compute the mean value for 2010.\n",
    "\n",
    "The life expectancy CSV file is available to you in the variable life_fname and the regions filename is available in the variable regions_fname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region\n",
      "America                       74.037350\n",
      "East Asia & Pacific           73.405750\n",
      "Europe & Central Asia         75.656387\n",
      "Middle East & North Africa    72.805333\n",
      "South Asia                    68.189750\n",
      "Sub-Saharan Africa            57.575080\n",
      "Name: 2010, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "life_fname = 'life_expectancy.csv'\n",
    "regions_fname = 'regions.csv'\n",
    "\n",
    "# Read life_fname into a DataFrame: life\n",
    "life = pd.read_csv(life_fname, index_col='Country')\n",
    "\n",
    "# Read regions_fname into a DataFrame: regions\n",
    "regions = pd.read_csv(regions_fname, index_col='Country')\n",
    "\n",
    "# Group life by regions['region']: life_by_region\n",
    "life_by_region = life.groupby(regions['region'])\n",
    "\n",
    "# Print the mean over the '2010' column of life_by_region\n",
    "print(life_by_region['2010'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby and aggregation\n",
    "### Computing multiple aggregates of multiple columns\n",
    "The .agg() method can be used with a tuple or list of aggregations as input. When applying multiple aggregations on multiple columns, the aggregated DataFrame has a multi-level column index.\n",
    "\n",
    "In this exercise, you're going to group passengers on the Titanic by 'pclass' and aggregate the 'age' and 'fare' columns by the functions 'max' and 'median'. You'll then use multi-level selection to find the oldest passenger per class and the median fare price per class.\n",
    "\n",
    "The DataFrame has been pre-loaded as titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         age             fare         \n",
      "         max median       max   median\n",
      "pclass                                \n",
      "1       80.0   39.0  512.3292  60.0000\n",
      "2       70.0   29.0   73.5000  15.0458\n",
      "3       74.0   24.0   69.5500   8.0500\n"
     ]
    }
   ],
   "source": [
    "# Group titanic by 'pclass': by_class\n",
    "by_class = titanic.groupby('pclass')\n",
    "\n",
    "# Select 'age' and 'fare'\n",
    "by_class_sub = by_class[['age','fare']]\n",
    "\n",
    "# Aggregate by_class_sub by 'max' and 'median': aggregated\n",
    "aggregated = by_class_sub.agg(['max', 'median'])\n",
    "\n",
    "print(aggregated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pclass\n",
      "1    80.0\n",
      "2    70.0\n",
      "3    74.0\n",
      "Name: (age, max), dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print the maximum age in each class\n",
    "print(aggregated.loc[:, ('age','max')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pclass\n",
      "1    60.0000\n",
      "2    15.0458\n",
      "3     8.0500\n",
      "Name: (fare, median), dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print the median fare in each class\n",
    "print(aggregated.loc[:, ('fare', 'median')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating on index levels/fields\n",
    "\n",
    "If you have a DataFrame with a multi-level row index, the individual levels can be used to perform the groupby. This allows advanced aggregation techniques to be applied along one or more levels in the index and across one or more columns.\n",
    "\n",
    "In this exercise you'll use the full Gapminder dataset which contains yearly values of life expectancy, population, child mortality (per 1,000) and per capita gross domestic product (GDP) for every country in the world from 1964 to 2013.\n",
    "\n",
    "Your job is to create a multi-level DataFrame of the columns 'Year', 'Region' and 'Country'. Next you'll group the DataFrame by the 'Year' and 'Region' levels. Finally, you'll apply a dictionary aggregation to compute the total population, spread of per capita GDP values and average child mortality rate.\n",
    "\n",
    "The Gapminder CSV file is is available as 'gapminder.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   population  child_mortality       gdp\n",
      "Year region                                                             \n",
      "2013 America                     9.629087e+08        17.745833   49634.0\n",
      "     East Asia & Pacific         2.244209e+09        22.285714  134744.0\n",
      "     Europe & Central Asia       8.968788e+08         9.831875   86418.0\n",
      "     Middle East & North Africa  4.030504e+08        20.221500  128676.0\n",
      "     South Asia                  1.701241e+09        46.287500   11469.0\n",
      "     Sub-Saharan Africa          8.929572e+08        76.897447   32035.0\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame and sort the index: gapminder\n",
    "gapminder = pd.read_csv('gapminder.csv', index_col=['Year', 'region', 'Country']).sort_index()\n",
    "\n",
    "# Group gapminder by 'Year' and 'region': by_year_region\n",
    "by_year_region = gapminder.groupby(level=['Year', 'region'])\n",
    "\n",
    "# Define the function to compute spread: spread\n",
    "def spread(series):\n",
    "    return series.max() - series.min()\n",
    "\n",
    "# Create the dictionary: aggregator\n",
    "aggregator = {'population':'sum', 'child_mortality':'mean', 'gdp':spread}\n",
    "\n",
    "# Aggregate by_year_region using the dictionary: aggregated\n",
    "aggregated = by_year_region.agg(aggregator)\n",
    "\n",
    "# Print the last 6 entries of aggregated \n",
    "print(aggregated.tail(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping on a function of the index\n",
    "\n",
    "Groubpy operations can also be performed on transformations of the index values. In the case of a DateTimeIndex, we can extract portions of the datetime over which to group.\n",
    "\n",
    "In this exercise you'll read in a set of sample sales data from February 2015 and assign the 'Date' column as the index. Your job is to group the sales data by the day of the week and aggregate the sum of the 'Units' column.\n",
    "\n",
    "Is there a day of the week that is more popular for customers? To find out, you're going to use .strftime('%a') to transform the index datetime values to abbreviated days of the week.\n",
    "\n",
    "The sales data CSV file is available to you as 'sales.csv'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon    48\n",
      "Sat     7\n",
      "Thu    59\n",
      "Tue    13\n",
      "Wed    48\n",
      "Name: Units, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read file: sales\n",
    "sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Create a groupby object: by_day\n",
    "by_day = sales.groupby(sales.index.strftime('%a'))\n",
    "\n",
    "# Create sum: units_sum\n",
    "units_sum = by_day.Units.sum()\n",
    "\n",
    "# Print units_sum\n",
    "print(units_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby and transformation\n",
    "### Detecting outliers with Z-Scores\n",
    "As Dhavide demonstrated in the video using the zscore function, you can apply a .transform() method after grouping to apply a function to groups of data independently. The z-score is also useful to find outliers: a z-score value of +/- 3 is generally considered to be an outlier.\n",
    "\n",
    "In this example, you're going to normalize the Gapminder data in 2010 for life expectancy and fertility by the z-score per region. Using boolean indexing, you will filter out countries that have high fertility rates and low life expectancy for their region.\n",
    "\n",
    "The Gapminder DataFrame for 2010 indexed by 'Country' is provided for you as gapminder_2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gapminder = pd.read_csv('gapminder.csv', index_col='Country')\n",
    "gapminder_2010 = gapminder[gapminder.Year == 2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Year  fertility    life  population  child_mortality     gdp  \\\n",
      "Country                                                                     \n",
      "Guatemala    2010      3.974  71.100  14388929.0             34.5  6849.0   \n",
      "Haiti        2010      3.350  45.000   9993247.0            208.8  1518.0   \n",
      "Tajikistan   2010      3.780  66.830   6878637.0             52.6  2110.0   \n",
      "Timor-Leste  2010      6.237  65.952   1124355.0             63.8  1777.0   \n",
      "\n",
      "                            region  \n",
      "Country                             \n",
      "Guatemala                  America  \n",
      "Haiti                      America  \n",
      "Tajikistan   Europe & Central Asia  \n",
      "Timor-Leste    East Asia & Pacific  \n"
     ]
    }
   ],
   "source": [
    "# Import zscore\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Group gapminder_2010: standardized\n",
    "standardized = gapminder_2010.groupby('region')['life', 'fertility'].transform(zscore)\n",
    "\n",
    "# Construct a Boolean Series to identify outliers: outliers\n",
    "outliers = ((standardized['life'] < -3) | (standardized['fertility'] > 3))\n",
    "\n",
    "# Filter gapminder_2010 by the outliers: gm_outliers\n",
    "gm_outliers = gapminder_2010.loc[outliers]\n",
    "\n",
    "# Print gm_outliers\n",
    "print(gm_outliers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing data (imputation) by group\n",
    "\n",
    "Many statistical and machine learning packages cannot determine the best action to take when missing data entries are encountered. Dealing with missing data is natural in pandas (both in using the default behavior and in defining a custom behavior). In Chapter 1, you practiced using the .dropna() method to drop missing values. Now, you will practice imputing missing values. You can use .groupby() and .transform() to fill missing data appropriately for each group.\n",
    "\n",
    "Your job is to fill in missing 'age' values for passengers on the Titanic with the median age from their 'gender' and 'pclass'. To do this, you'll group by the 'sex' and 'pclass' columns and transform each group with a custom function to call .fillna() and impute the median value.\n",
    "\n",
    "The DataFrame has been pre-loaded as titanic. Explore it in the IPython Shell by printing the output of titanic.tail(10). Notice in particular the NaNs in the 'age' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pclass  survived                                     name     sex   age  \\\n",
      "1299       3         0                      Yasbeck, Mr. Antoni    male  27.0   \n",
      "1300       3         1  Yasbeck, Mrs. Antoni (Selini Alexander)  female  15.0   \n",
      "1301       3         0                     Youseff, Mr. Gerious    male  45.5   \n",
      "1302       3         0                        Yousif, Mr. Wazli    male  25.0   \n",
      "1303       3         0                    Yousseff, Mr. Gerious    male  25.0   \n",
      "1304       3         0                     Zabour, Miss. Hileni  female  14.5   \n",
      "1305       3         0                    Zabour, Miss. Thamine  female  22.0   \n",
      "1306       3         0                Zakarian, Mr. Mapriededer    male  26.5   \n",
      "1307       3         0                      Zakarian, Mr. Ortin    male  27.0   \n",
      "1308       3         0                       Zimmerman, Mr. Leo    male  29.0   \n",
      "\n",
      "      sibsp  parch  ticket     fare cabin embarked boat   body home.dest  \n",
      "1299      1      0    2659  14.4542   NaN        C    C    NaN       NaN  \n",
      "1300      1      0    2659  14.4542   NaN        C  NaN    NaN       NaN  \n",
      "1301      0      0    2628   7.2250   NaN        C  NaN  312.0       NaN  \n",
      "1302      0      0    2647   7.2250   NaN        C  NaN    NaN       NaN  \n",
      "1303      0      0    2627  14.4583   NaN        C  NaN    NaN       NaN  \n",
      "1304      1      0    2665  14.4542   NaN        C  NaN  328.0       NaN  \n",
      "1305      1      0    2665  14.4542   NaN        C  NaN    NaN       NaN  \n",
      "1306      0      0    2656   7.2250   NaN        C  NaN  304.0       NaN  \n",
      "1307      0      0    2670   7.2250   NaN        C  NaN    NaN       NaN  \n",
      "1308      0      0  315082   7.8750   NaN        S  NaN    NaN       NaN  \n"
     ]
    }
   ],
   "source": [
    "titanic = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Create a groupby object: by_sex_class\n",
    "by_sex_class = titanic.groupby(['sex', 'pclass'])\n",
    "\n",
    "# Write a function that imputes median\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "# Impute age and assign to titanic['age']\n",
    "titanic.age = by_sex_class.age.transform(impute_median)\n",
    "\n",
    "# Print the output of titanic.tail(10)\n",
    "print(titanic.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other transformations with .apply\n",
    "\n",
    "The .apply() method when used on a groupby object performs an arbitrary function on each of the groups. These functions can be aggregations, transformations or more complex workflows. The .apply() method will then combine the results in an intelligent way.\n",
    "\n",
    "In this exercise, you're going to analyze economic disparity within regions of the world using the Gapminder data set for 2010. To do this you'll define a function to compute the aggregate spread of per capita GDP in each region and the individual country's z-score of the regional per capita GDP. You'll then select three countries - United States, Great Britain and China - to see a summary of the regional GDP and that country's z-score against the regional mean.\n",
    "\n",
    "The 2010 Gapminder DataFrame is provided for you as gapminder_2010. Pandas has been imported as pd.\n",
    "\n",
    "The following function has been defined for your use:\n",
    "```\n",
    "def disparity(gr):\n",
    "    # Compute the spread of gr['gdp']: s\n",
    "    s = gr['gdp'].max() - gr['gdp'].min()\n",
    "    # Compute the z-score of gr['gdp'] as (gr['gdp']-gr['gdp'].mean())/gr['gdp'].std(): z\n",
    "    z = (gr['gdp'] - gr['gdp'].mean())/gr['gdp'].std()\n",
    "    # Return a DataFrame with the inputs {'z(gdp)':z, 'regional spread(gdp)':s}\n",
    "    return pd.DataFrame({'z(gdp)':z , 'regional spread(gdp)':s})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disparity(gr):\n",
    "    # Compute the spread of gr['gdp']: s\n",
    "    s = gr['gdp'].max() - gr['gdp'].min()\n",
    "    # Compute the z-score of gr['gdp'] as (gr['gdp']-gr['gdp'].mean())/gr['gdp'].std(): z\n",
    "    z = (gr['gdp'] - gr['gdp'].mean())/gr['gdp'].std()\n",
    "    # Return a DataFrame with the inputs {'z(gdp)':z, 'regional spread(gdp)':s}\n",
    "    return pd.DataFrame({'z(gdp)':z , 'regional spread(gdp)':s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                regional spread(gdp)    z(gdp)\n",
      "Country                                       \n",
      "United States                47855.0  3.013374\n",
      "United Kingdom               89037.0  0.572873\n",
      "China                        96993.0 -0.432756\n"
     ]
    }
   ],
   "source": [
    "# Group gapminder_2010 by 'region': regional\n",
    "regional = gapminder_2010.groupby('region')\n",
    "\n",
    "# Apply the disparity function on regional: reg_disp\n",
    "reg_disp = regional.apply(disparity)\n",
    "\n",
    "# Print the disparity of 'United States', 'United Kingdom', and 'China'\n",
    "print(reg_disp.loc[['United States', 'United Kingdom', 'China']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby and filtering\n",
    "### Grouping and filtering with .apply()\n",
    "By using .apply(), you can write functions that filter rows within groups. The .apply() method will handle the iteration over individual groups and then re-combine them back into a Series or DataFrame.\n",
    "\n",
    "In this exercise you'll take the Titanic data set and analyze survival rates from the 'C' deck, which contained the most passengers. To do this you'll group the dataset by 'sex' and then use the .apply() method on a provided user defined function which calculates the mean survival rates on the 'C' deck:\n",
    "```\n",
    "def c_deck_survival(gr):\n",
    "\n",
    "    c_passengers = gr['cabin'].str.startswith('C').fillna(False)\n",
    "\n",
    "    return gr.loc[c_passengers, 'survived'].mean()\n",
    "```\n",
    "The DataFrame has been pre-loaded as titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def c_deck_survival(gr):\n",
    "    c_passengers = gr['cabin'].str.startswith('C').fillna(False)\n",
    "    return gr.loc[c_passengers, 'survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "female    0.913043\n",
      "male      0.312500\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a groupby object using titanic over the 'sex' column: by_sex\n",
    "by_sex = titanic.groupby('sex')\n",
    "\n",
    "# Call by_sex.apply with the function c_deck_survival and print the result\n",
    "c_surv_by_sex = by_sex.apply(c_deck_survival)\n",
    "\n",
    "# Print the survival rates\n",
    "print(c_surv_by_sex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and filtering with .filter()\n",
    "You can use groupby with the .filter() method to remove whole groups of rows from a DataFrame based on a boolean condition.\n",
    "\n",
    "In this exercise, you'll take the February sales data and remove entries from companies that purchased less than 35 Units in the whole month.\n",
    "\n",
    "First, you'll identify how many units each company bought for verification. Next you'll use the .filter() method after grouping by 'Company' to remove all rows belonging to companies whose sum over the 'Units' column was less than 35. Finally, verify that the three companies whose total Units purchased were less than 35 have been filtered out from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company\n",
      "Acme Coporation    34\n",
      "Hooli              30\n",
      "Initech            30\n",
      "Mediacore          45\n",
      "Streeplex          36\n",
      "Name: Units, dtype: int64\n",
      "                       Company   Product  Units\n",
      "Date                                           \n",
      "2015-02-02 21:00:00  Mediacore  Hardware      9\n",
      "2015-02-04 15:30:00  Streeplex  Software     13\n",
      "2015-02-09 09:00:00  Streeplex   Service     19\n",
      "2015-02-09 13:00:00  Mediacore  Software      7\n",
      "2015-02-19 11:00:00  Mediacore  Hardware     16\n",
      "2015-02-19 16:00:00  Mediacore   Service     10\n",
      "2015-02-21 05:00:00  Mediacore  Software      3\n",
      "2015-02-26 09:00:00  Streeplex   Service      4\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame: sales\n",
    "sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Group sales by 'Company': by_company\n",
    "by_company = sales.groupby('Company')\n",
    "\n",
    "# Compute the sum of the 'Units' of by_company: by_com_sum\n",
    "by_com_sum = by_company.Units.sum()\n",
    "print(by_com_sum)\n",
    "\n",
    "# Filter 'Units' where the sum is > 35: by_com_filt\n",
    "by_com_filt = by_company.filter(lambda g: g['Units'].sum() > 35)\n",
    "print(by_com_filt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and grouping with .map()\n",
    "You have seen how to group by a column, or by multiple columns. Sometimes, you may instead want to group by a function/transformation of a column. The key here is that the Series is indexed the same way as the DataFrame. You can also mix and match column grouping with Series grouping.\n",
    "\n",
    "In this exercise your job is to investigate survival rates of passengers on the Titanic by 'age' and 'pclass'. In particular, the goal is to find out what fraction of children under 10 survived in each 'pclass'. You'll do this by first creating a boolean array where True is passengers under 10 years old and False is passengers over 10. You'll use .map() to change these values to strings.\n",
    "\n",
    "Finally, you'll group by the under 10 series and the 'pclass' column and aggregate the 'survived' column. The 'survived' column has the value 1 if the passenger survived and 0 otherwise. The mean of the 'survived' column is the fraction of passengers who lived.\n",
    "\n",
    "The DataFrame has been pre-loaded for you as titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "over 10     0.366748\n",
      "under 10    0.609756\n",
      "Name: survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the Boolean Series: under10\n",
    "under10 = (titanic.age < 10).map({True: 'under 10', False: 'over 10'})\n",
    "\n",
    "# Group by under10 and compute the survival rate\n",
    "survived_mean_1 = titanic.groupby(under10)['survived'].mean()\n",
    "print(survived_mean_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age       pclass\n",
      "over 10   1         0.617555\n",
      "          2         0.380392\n",
      "          3         0.238897\n",
      "under 10  1         0.750000\n",
      "          2         1.000000\n",
      "          3         0.446429\n",
      "Name: survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Group by under10 and pclass and compute the survival rate\n",
    "survived_mean_2 = titanic.groupby([under10,'pclass'])['survived'].mean()\n",
    "print(survived_mean_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
