{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating data\n",
    "\n",
    "## Appending & concatenating Series\n",
    "\n",
    "### Appending pandas Series\n",
    "In this exercise, you'll load sales data from the months January, February, and March into DataFrames. Then, you'll extract Series with the 'Units' column from each and append them together with method chaining using .append().\n",
    "\n",
    "To check that the stacking worked, you'll print slices from these Series, and finally, you'll add the result to figure out the total units sold in the first quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2015-01-27 07:11:55    18\n",
      "2015-02-02 08:33:01     3\n",
      "2015-02-02 20:54:49     9\n",
      "Name: Units, dtype: int64\n",
      "Date\n",
      "2015-02-26 08:57:45     4\n",
      "2015-02-26 08:58:51     1\n",
      "2015-03-06 10:11:45    17\n",
      "2015-03-06 02:03:56    17\n",
      "Name: Units, dtype: int64\n",
      "642\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load 'sales-jan-2015.csv' into a DataFrame: jan\n",
    "jan = pd.read_csv('sales-jan-2015.csv', parse_dates=True, index_col='Date')\n",
    "\n",
    "# Load 'sales-feb-2015.csv' into a DataFrame: feb\n",
    "feb = pd.read_csv('sales-feb-2015.csv', parse_dates=True, index_col='Date')\n",
    "\n",
    "# Load 'sales-mar-2015.csv' into a DataFrame: mar\n",
    "mar = pd.read_csv('sales-mar-2015.csv', parse_dates=True, index_col='Date')\n",
    "\n",
    "# Extract the 'Units' column from jan: jan_units\n",
    "jan_units = jan['Units']\n",
    "\n",
    "# Extract the 'Units' column from feb: feb_units\n",
    "feb_units = feb['Units']\n",
    "\n",
    "# Extract the 'Units' column from mar: mar_units\n",
    "mar_units = mar['Units']\n",
    "\n",
    "# Append feb_units and then mar_units to jan_units: quarter1\n",
    "quarter1 = jan_units.append(feb_units).append(mar_units)\n",
    "\n",
    "# Print the first slice from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "\n",
    "# Print the second slice from quarter1\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])\n",
    "\n",
    "# Compute & print total sales in quarter1\n",
    "print(quarter1.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating pandas Series along row axis\n",
    "Having learned how to append Series, you'll now learn how to achieve the same result by concatenating Series instead. You'll continue to work with the sales data you've seen previously. This time, the DataFrames jan, feb, and mar have been pre-loaded.\n",
    "\n",
    "Your job is to use pd.concat() with a list of Series to achieve the same result that you would get by chaining calls to .append().\n",
    "\n",
    "You may be wondering about the difference between pd.concat() and pandas' .append() method. One way to think of the difference is that .append() is a specific case of a concatenation, while pd.concat() gives you more flexibility, as you'll see in later exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2015-01-27 07:11:55    18\n",
      "2015-02-02 08:33:01     3\n",
      "2015-02-02 20:54:49     9\n",
      "Name: Units, dtype: int64\n",
      "Date\n",
      "2015-02-26 08:57:45     4\n",
      "2015-02-26 08:58:51     1\n",
      "2015-03-06 10:11:45    17\n",
      "2015-03-06 02:03:56    17\n",
      "Name: Units, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty list: units\n",
    "units = []\n",
    "\n",
    "# Build the list of Series\n",
    "for month in [jan, feb, mar]:\n",
    "    units.append(month['Units'])\n",
    "\n",
    "# Concatenate the list: quarter1\n",
    "quarter1 = pd.concat(units, axis='rows')\n",
    "\n",
    "# Print slices from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending & concatenating DataFrames\n",
    "### Appending DataFrames with ignore_index\n",
    "In this exercise, you'll use the Baby Names Dataset (from data.gov) again. This time, both DataFrames names_1981 and names_1881 are loaded without specifying an Index column (so the default Indexes for both are RangeIndexes).\n",
    "\n",
    "You'll use the DataFrame .append() method to make a DataFrame combined_names. To distinguish rows from the original two DataFrames, you'll add a 'year' column to each with the year (1881 or 1981 in this case). In addition, you'll specify ignore_index=True so that the index values are not used along the concatenation axis. The resulting axis will instead be labeled 0, 1, ..., n-1, which is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names_1981 = pd.read_csv('names1981.csv', header=None, names=['name','gender','count'])\n",
    "names_1881 = pd.read_csv('names1881.csv', header=None, names=['name','gender','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19455, 4)\n",
      "(1935, 4)\n",
      "(21390, 4)\n",
      "         name gender  count  year\n",
      "1283   Morgan      M     23  1881\n",
      "2096   Morgan      F   1769  1981\n",
      "14390  Morgan      M    766  1981\n"
     ]
    }
   ],
   "source": [
    "# Add 'year' column to names_1881 and names_1981\n",
    "names_1881['year'] = 1881\n",
    "names_1981['year'] = 1981\n",
    "\n",
    "# Append names_1981 after names_1881 with ignore_index=True: combined_names\n",
    "combined_names = names_1881.append(names_1981, ignore_index=True)\n",
    "\n",
    "# Print shapes of names_1981, names_1881, and combined_names\n",
    "print(names_1981.shape)\n",
    "print(names_1881.shape)\n",
    "print(combined_names.shape)\n",
    "\n",
    "# Print all rows that contain the name 'Morgan'\n",
    "print(combined_names[combined_names.name == 'Morgan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating pandas DataFrames along column axis\n",
    "The function pd.concat() can concatenate DataFrames horizontally as well as vertically (vertical is the default). To make the DataFrames stack horizontally, you have to specify the keyword argument axis=1 or axis='columns'.\n",
    "\n",
    "In this exercise, you'll use weather data with maximum and mean daily temperatures sampled at different rates (quarterly versus monthly). You'll concatenate the rows of both and see that, where rows are missing in the coarser DataFrame, null values are inserted in the concatenated DataFrame. This corresponds to an outer join (which you will explore in more detail in later exercises).\n",
    "\n",
    "The files 'quarterly_max_temp.csv' and 'monthly_mean_temp.csv' have been pre-loaded into the DataFrames weather_max and weather_mean respectively, and pandas has been imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Max TemperatureF\n",
      "Month                  \n",
      "Jan                  68\n",
      "Apr                  89\n",
      "Jul                  91\n",
      "Oct                  84\n",
      "       Mean TemperatureF\n",
      "Month                   \n",
      "Apr            53.100000\n",
      "Aug            70.000000\n",
      "Dec            34.935484\n",
      "Feb            28.714286\n",
      "Jan            32.354839\n",
      "Jul            72.870968\n",
      "Jun            70.133333\n",
      "Mar            35.000000\n",
      "May            62.612903\n",
      "Nov            39.800000\n",
      "Oct            55.451613\n",
      "Sep            63.766667\n"
     ]
    }
   ],
   "source": [
    "weather_max = pd.read_csv('quarterly_max_temp.csv', index_col='Month')\n",
    "weather_mean = pd.read_csv('monthly_mean_temp.csv', index_col='Month')\n",
    "print(weather_max)\n",
    "print(weather_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Max TemperatureF  Mean TemperatureF\n",
      "Apr              89.0          53.100000\n",
      "Aug               NaN          70.000000\n",
      "Dec               NaN          34.935484\n",
      "Feb               NaN          28.714286\n",
      "Jan              68.0          32.354839\n",
      "Jul              91.0          72.870968\n",
      "Jun               NaN          70.133333\n",
      "Mar               NaN          35.000000\n",
      "May               NaN          62.612903\n",
      "Nov               NaN          39.800000\n",
      "Oct              84.0          55.451613\n",
      "Sep               NaN          63.766667\n"
     ]
    }
   ],
   "source": [
    "# Concatenate weather_max and weather_mean horizontally: weather\n",
    "weather = pd.concat([weather_max, weather_mean], axis=1)\n",
    "\n",
    "# Print weather\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading multiple files to build a DataFrame\n",
    "It is often convenient to build a large DataFrame by parsing many files as DataFrames and concatenating them all at once. You'll do this here with three files, but, in principle, this approach can be used to combine data from dozens or hundreds of files.\n",
    "\n",
    "Here, you'll work with DataFrames compiled from The Guardian's Olympic medal dataset.\n",
    "\n",
    "pandas has been imported as pd and two lists have been pre-loaded: An empty list called medals, and medal_types, which contains the strings 'bronze', 'silver', and 'gold'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                bronze  silver    gold\n",
      "France           475.0   461.0     NaN\n",
      "Germany          454.0     NaN   407.0\n",
      "Italy              NaN   394.0   460.0\n",
      "Soviet Union     584.0   627.0   838.0\n",
      "United Kingdom   505.0   591.0   498.0\n",
      "United States   1052.0  1195.0  2088.0\n"
     ]
    }
   ],
   "source": [
    "medals = []\n",
    "medal_types = ['bronze', 'silver', 'gold']\n",
    "\n",
    "for medal in medal_types:\n",
    "\n",
    "    # Create the file name: file_name\n",
    "    file_name = \"%s_top5.csv\" % medal\n",
    "    \n",
    "    # Create list of column names: columns\n",
    "    columns = ['Country', medal]\n",
    "    \n",
    "    # Read file_name into a DataFrame: df\n",
    "    medal_df = pd.read_csv(file_name, header=0, index_col='Country', names=columns)\n",
    "\n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "\n",
    "# Concatenate medals horizontally: medals\n",
    "medals = pd.concat(medals, axis='columns')\n",
    "\n",
    "# Print medals\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation, keys, & MultiIndexes\n",
    "### Concatenating vertically to get MultiIndexed rows\n",
    "When stacking a sequence of DataFrames vertically, it is sometimes desirable to construct a MultiIndex to indicate the DataFrame from which each row originated. This can be done by specifying the keys parameter in the call to pd.concat(), which generates a hierarchical index with the labels from keys as the outermost index label. So you don't have to rename the columns of each DataFrame as you load it. Instead, only the Index column needs to be specified.\n",
    "\n",
    "Here, you'll continue working with DataFrames compiled from The Guardian's Olympic medal dataset. Once again, pandas has been imported as pd and two lists have been pre-loaded: An empty list called medals, and medal_types, which contains the strings 'bronze', 'silver', and 'gold'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Total\n",
      "       Country               \n",
      "bronze United States   1052.0\n",
      "       Soviet Union     584.0\n",
      "       United Kingdom   505.0\n",
      "       France           475.0\n",
      "       Germany          454.0\n",
      "silver United States   1195.0\n",
      "       Soviet Union     627.0\n",
      "       United Kingdom   591.0\n",
      "       France           461.0\n",
      "       Italy            394.0\n",
      "gold   United States   2088.0\n",
      "       Soviet Union     838.0\n",
      "       United Kingdom   498.0\n",
      "       Italy            460.0\n",
      "       Germany          407.0\n"
     ]
    }
   ],
   "source": [
    "medals = []\n",
    "medal_types = ['bronze', 'silver', 'gold']\n",
    "\n",
    "for medal in medal_types:\n",
    "\n",
    "    file_name = \"%s_top5.csv\" % medal\n",
    "    \n",
    "    # Read file_name into a DataFrame: medal_df\n",
    "    medal_df = pd.read_csv(file_name, index_col='Country')\n",
    "    \n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "    \n",
    "# Concatenate medals: medals\n",
    "medals = pd.concat(medals, keys=['bronze', 'silver', 'gold'])\n",
    "\n",
    "# Print medals in entirety\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing MultiIndexed DataFrames\n",
    "This exercise picks up where the last ended (again using The Guardian's Olympic medal dataset).\n",
    "\n",
    "You are provided with the MultiIndexed DataFrame as produced at the end of the preceding exercise. Your task is to sort the DataFrame and to use the pd.IndexSlice to extract specific slices. Check out this exercise from Manipulating DataFrames with pandas to refresh your memory on how to deal with MultiIndexed DataFrames.\n",
    "\n",
    "pandas has been imported for you as pd and the DataFrame medals is already in your namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total    454.0\n",
      "Name: (bronze, Germany), dtype: float64\n",
      "                 Total\n",
      "Country               \n",
      "France           461.0\n",
      "Italy            394.0\n",
      "Soviet Union     627.0\n",
      "United Kingdom   591.0\n",
      "United States   1195.0\n",
      "                       Total\n",
      "       Country              \n",
      "bronze United Kingdom  505.0\n",
      "silver United Kingdom  591.0\n",
      "gold   United Kingdom  498.0\n"
     ]
    }
   ],
   "source": [
    "# Sort the entries of medals: medals_sorted\n",
    "medals_sorted = medals.sort_index(level=0)\n",
    "\n",
    "# Print the number of Bronze medals won by Germany\n",
    "print(medals_sorted.loc[('bronze','Germany')])\n",
    "\n",
    "# Print data about silver medals\n",
    "print(medals_sorted.loc['silver'])\n",
    "\n",
    "# Create alias for pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Print all the data on medals won by the United Kingdom\n",
    "print(medals_sorted.loc[idx[:,'United Kingdom'], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating horizontally to get MultiIndexed columns\n",
    "It is also possible to construct a DataFrame with hierarchically indexed columns. For this exercise, you'll start with pandas imported and a list of three DataFrames called dataframes. All three DataFrames contain 'Company', 'Product', and 'Units' columns with a 'Date' column as the index pertaining to sales transactions during the month of February, 2015. The first DataFrame describes Hardware transactions, the second describes Software transactions, and the third, Service transactions.\n",
    "\n",
    "Your task is to concatenate the DataFrames horizontally and to create a MultiIndex on the columns. From there, you can summarize the resulting DataFrame and slice some information from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                             Company   Product  Units\n",
       " Date                                                 \n",
       " 2015-02-04 21:52:45  Acme Coporation  Hardware     14\n",
       " 2015-02-07 22:58:10  Acme Coporation  Hardware      1\n",
       " 2015-02-19 10:59:33        Mediacore  Hardware     16\n",
       " 2015-02-02 20:54:49        Mediacore  Hardware      9\n",
       " 2015-02-21 20:41:47            Hooli  Hardware      3,\n",
       "                        Company  Product  Units\n",
       " Date                                          \n",
       " 2015-02-26 08:57:45  Streeplex  Service      4\n",
       " 2015-02-25 00:29:00    Initech  Service     10\n",
       " 2015-02-09 08:57:30  Streeplex  Service     19\n",
       " 2015-02-26 08:58:51  Streeplex  Service      1\n",
       " 2015-02-05 22:05:03      Hooli  Service     10\n",
       " 2015-02-19 16:02:58  Mediacore  Service     10,\n",
       "                              Company   Product  Units\n",
       " Date                                                 \n",
       " 2015-02-16 12:09:19            Hooli  Software     10\n",
       " 2015-02-03 14:14:18          Initech  Software     13\n",
       " 2015-02-02 08:33:01            Hooli  Software      3\n",
       " 2015-02-05 01:53:06  Acme Coporation  Software     19\n",
       " 2015-02-11 20:03:08          Initech  Software      7\n",
       " 2015-02-09 13:09:55        Mediacore  Software      7\n",
       " 2015-02-11 22:50:44            Hooli  Software      4\n",
       " 2015-02-04 15:36:29        Streeplex  Software     13\n",
       " 2015-02-21 05:01:26        Mediacore  Software      3]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feb = pd.read_csv('sales-feb-2015.csv', parse_dates=True, index_col='Date')\n",
    "dataframes = [group for name, group in feb.groupby('Product')]\n",
    "dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 20 entries, 2015-02-02 08:33:01 to 2015-02-26 08:58:51\n",
      "Data columns (total 9 columns):\n",
      "(Hardware, Company)    5 non-null object\n",
      "(Hardware, Product)    5 non-null object\n",
      "(Hardware, Units)      5 non-null float64\n",
      "(Service, Company)     6 non-null object\n",
      "(Service, Product)     6 non-null object\n",
      "(Service, Units)       6 non-null float64\n",
      "(Software, Company)    9 non-null object\n",
      "(Software, Product)    9 non-null object\n",
      "(Software, Units)      9 non-null float64\n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 1.6+ KB\n",
      "None\n",
      "                            Hardware Service         Software\n",
      "                             Company Company          Company\n",
      "Date                                                         \n",
      "2015-02-02 08:33:01              NaN     NaN            Hooli\n",
      "2015-02-02 20:54:49        Mediacore     NaN              NaN\n",
      "2015-02-03 14:14:18              NaN     NaN          Initech\n",
      "2015-02-04 15:36:29              NaN     NaN        Streeplex\n",
      "2015-02-04 21:52:45  Acme Coporation     NaN              NaN\n",
      "2015-02-05 01:53:06              NaN     NaN  Acme Coporation\n",
      "2015-02-05 22:05:03              NaN   Hooli              NaN\n",
      "2015-02-07 22:58:10  Acme Coporation     NaN              NaN\n"
     ]
    }
   ],
   "source": [
    "# Concatenate dataframes: february\n",
    "february = pd.concat(dataframes, axis=1, keys=['Hardware', 'Service', 'Software'])\n",
    "\n",
    "# Print february.info()\n",
    "print(february.info())\n",
    "\n",
    "# Assign pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Create the slice: slice_2_8\n",
    "slice_2_8 = february.loc['Feb. 2, 2015':'Feb. 8, 2015', idx[:, 'Company']]\n",
    "\n",
    "# Print slice_2_8\n",
    "print(slice_2_8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating DataFrames from a dict\n",
    "\n",
    "You're now going to revisit the sales data you worked with earlier in the chapter. Three DataFrames jan, feb, and mar have been pre-loaded for you. Your task is to aggregate the sum of all sales over the 'Company' column into a single DataFrame. You'll do this by constructing a dictionary of these DataFrames and then concatenating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Units\n",
      "         Company               \n",
      "february Acme Coporation     34\n",
      "         Hooli               30\n",
      "         Initech             30\n",
      "         Mediacore           45\n",
      "         Streeplex           37\n",
      "january  Acme Coporation     76\n",
      "         Hooli               70\n",
      "         Initech             37\n",
      "         Mediacore           15\n",
      "         Streeplex           50\n",
      "march    Acme Coporation      5\n",
      "         Hooli               37\n",
      "         Initech             68\n",
      "         Mediacore           68\n",
      "         Streeplex           40\n"
     ]
    }
   ],
   "source": [
    "# Make the list of tuples: month_list\n",
    "month_list = [('january', jan), ('february', feb), ('march', mar)]\n",
    "\n",
    "# Create an empty dictionary: month_dict\n",
    "month_dict = {}\n",
    "\n",
    "for month_name, month_data in month_list:\n",
    "\n",
    "    # Group month_data: month_dict[month_name]\n",
    "    month_dict[month_name] = month_data.groupby('Company').sum()\n",
    "\n",
    "# Concatenate data in month_dict: sales\n",
    "sales = pd.concat(month_dict)\n",
    "\n",
    "# Print sales\n",
    "print(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Units\n",
      "         Company         \n",
      "february Mediacore     45\n",
      "january  Mediacore     15\n",
      "march    Mediacore     68\n"
     ]
    }
   ],
   "source": [
    "# Print all sales by Mediacore\n",
    "idx = pd.IndexSlice\n",
    "print(sales.loc[idx[:, 'Mediacore'], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer & inner joins\n",
    "\n",
    "### Concatenating DataFrames with inner join\n",
    "\n",
    "Here, you'll continue working with DataFrames compiled from The Guardian's Olympic medal dataset.\n",
    "\n",
    "The DataFrames bronze, silver, and gold have been pre-loaded for you.\n",
    "\n",
    "Your task is to compute an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                bronze  silver    gold\n",
      "                 Total   Total   Total\n",
      "Country                               \n",
      "United States   1052.0  1195.0  2088.0\n",
      "Soviet Union     584.0   627.0   838.0\n",
      "United Kingdom   505.0   591.0   498.0\n"
     ]
    }
   ],
   "source": [
    "bronze = pd.read_csv('bronze_top5.csv', index_col='Country')\n",
    "silver = pd.read_csv('silver_top5.csv', index_col='Country')\n",
    "gold = pd.read_csv('gold_top5.csv', index_col='Country')\n",
    "\n",
    "# Create the list of DataFrames: medal_list\n",
    "medal_list = [bronze, silver, gold]\n",
    "\n",
    "# Concatenate medal_list horizontally using an inner join: medals\n",
    "medals = pd.concat(medal_list, keys=['bronze', 'silver', 'gold'], axis=1, join='inner')\n",
    "\n",
    "# Print medals\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling & concatenating DataFrames with inner join\n",
    "In this exercise, you'll compare the historical 10-year GDP (Gross Domestic Product) growth in the US and in China. The data for the US starts in 1947 and is recorded quarterly; by contrast, the data for China starts in 1966 and is recorded annually.\n",
    "\n",
    "You'll need to use a combination of resampling and an inner join to align the index labels. You'll need an appropriate offset alias for resampling, and the method .resample() must be chained with some kind of aggregation method (.pct_change() and .last() in this case).\n",
    "\n",
    "pandas has been imported as pd, and the DataFrames china and us have been pre-loaded, with the output of china.head() and us.head() printed in the IPython Shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                China\n",
      "Year                 \n",
      "1961-01-01  49.557050\n",
      "1962-01-01  46.685179\n",
      "1963-01-01  50.097303\n",
      "1964-01-01  59.062255\n",
      "1965-01-01  69.709153\n"
     ]
    }
   ],
   "source": [
    "china = pd.read_csv('gdp_china.csv', parse_dates=True, index_col='Year', names=['Year', 'China'], header=1)\n",
    "print(china.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               US\n",
      "Year             \n",
      "1947-04-01  246.3\n",
      "1947-07-01  250.1\n",
      "1947-10-01  260.3\n",
      "1948-01-01  266.2\n",
      "1948-04-01  272.9\n"
     ]
    }
   ],
   "source": [
    "us = pd.read_csv('GDP.csv', parse_dates=True, index_col=0, names=['Year', 'US'], header=1)\n",
    "print(us.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               China        US\n",
      "Year                          \n",
      "1971-12-31  0.988860  1.073188\n",
      "1981-12-31  0.972048  1.749631\n",
      "1991-12-31  0.962528  0.922811\n",
      "2001-12-31  2.492511  0.720398\n",
      "2011-12-31  4.623958  0.460947\n",
      "2021-12-31  3.789936  0.377506\n"
     ]
    }
   ],
   "source": [
    "# Resample and tidy china: china_annual\n",
    "china_annual = china.resample('A').mean().pct_change(10).dropna()\n",
    "\n",
    "# Resample and tidy us: us_annual\n",
    "us_annual = us.resample('A').mean().pct_change(10).dropna()\n",
    "\n",
    "# Concatenate china_annual and us_annual: gdp\n",
    "gdp = pd.concat([china_annual, us_annual], axis=1, join='inner')\n",
    "\n",
    "# Resample gdp and print\n",
    "print(gdp.resample('10A').last())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
